{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Python Log Management: Using the Log Class\n",
    "\n",
    "Logging is a crucial aspect in software development as it allows for recording and monitoring the behavior of a program during execution. In Python, the logging module provides a wide range of functionalities for efficient logging.\n",
    "\n",
    "In this notebook, we'll explore the implementation of a simple Log class that simplifies the usage of the logging module. This class provides methods for logging messages of different severity levels such as DEBUG, INFO, WARNING, ERROR, and CRITICAL. Additionally, it offers flexibility by allowing the user to enable or disable logging, set the logging level, and choose between using the logging module or printing messages to the console.\n",
    "\n",
    "Let's dive in and see how to use the Log class effectively for managing logs in Python projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class Log:\n",
    "    _level = logging.INFO  # Default logging level\n",
    "    _use_logging = True  # Flag to control whether to use logging module or print to console\n",
    "\n",
    "    @staticmethod\n",
    "    def setLogLevel(level=logging.INFO):\n",
    "        \"\"\"\n",
    "        Set the logging level for the Log class.\n",
    "        \n",
    "        Args:\n",
    "            level (int): The logging level to be set.\n",
    "        \"\"\"\n",
    "        Log._level = level\n",
    "        logging.basicConfig(level=Log._level)\n",
    "\n",
    "    @staticmethod\n",
    "    def useLogging(flag=True):\n",
    "        \"\"\"\n",
    "        Enable or disable logging.\n",
    "        \n",
    "        Args:\n",
    "            flag (bool): If True, enable logging; if False, disable logging.\n",
    "        \"\"\"\n",
    "        Log._use_logging = flag\n",
    "\n",
    "    @staticmethod\n",
    "    def debug(message, *args):\n",
    "        \"\"\"\n",
    "        Log a debug message.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The debug message.\n",
    "            *args: Optional arguments to format the message.\n",
    "        \"\"\"\n",
    "        Log.__log(message, logging.DEBUG, *args)\n",
    "\n",
    "    @staticmethod\n",
    "    def info(message, *args):\n",
    "        \"\"\"\n",
    "        Log an info message.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The info message.\n",
    "            *args: Optional arguments to format the message.\n",
    "        \"\"\"\n",
    "        Log.__log(message, logging.INFO, *args)\n",
    "\n",
    "    @staticmethod\n",
    "    def warning(message, *args):\n",
    "        \"\"\"\n",
    "        Log a warning message.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The warning message.\n",
    "            *args: Optional arguments to format the message.\n",
    "        \"\"\"\n",
    "        Log.__log(message, logging.WARNING, *args)\n",
    "\n",
    "    @staticmethod\n",
    "    def error(message, *args):\n",
    "        \"\"\"\n",
    "        Log an error message.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The error message.\n",
    "            *args: Optional arguments to format the message.\n",
    "        \"\"\"\n",
    "        Log.__log(message, logging.ERROR, *args)\n",
    "\n",
    "    @staticmethod\n",
    "    def critical(message, *args):\n",
    "        \"\"\"\n",
    "        Log a critical message.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The critical message.\n",
    "            *args: Optional arguments to format the message.\n",
    "        \"\"\"\n",
    "        Log.__log(message, logging.CRITICAL, *args)\n",
    "\n",
    "    @staticmethod\n",
    "    def __log(message, method, *args):\n",
    "        \"\"\"\n",
    "        Internal method to log messages.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to be logged.\n",
    "            method (int): The logging method (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n",
    "            *args: Optional arguments to format the message.\n",
    "        \"\"\"\n",
    "        if Log._use_logging:\n",
    "            logging.log(method, message, *args)\n",
    "        else:\n",
    "            if method >= Log._level:\n",
    "                formatted_message = message % args if args else message\n",
    "                print(f\"[{Log._get_logging_level_name(method)}] {formatted_message}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_logging_level_name(method):\n",
    "        \"\"\"\n",
    "        Get the name of the logging level.\n",
    "        \n",
    "        Args:\n",
    "            method (int): The logging method (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n",
    "        \n",
    "        Returns:\n",
    "            str: The name of the logging level.\n",
    "        \"\"\"\n",
    "        if method == logging.DEBUG:\n",
    "            return \"DEBUG\"\n",
    "        elif method == logging.INFO:\n",
    "            return \"INFO\"\n",
    "        elif method == logging.WARNING:\n",
    "            return \"WARNING\"\n",
    "        elif method == logging.ERROR:\n",
    "            return \"ERROR\"\n",
    "        elif method == logging.CRITICAL:\n",
    "            return \"CRITICAL\"\n",
    "        else:\n",
    "            return \"UNKNOWN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Union\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(dim1: int, dim2: int, device) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
    "    Modified from: \n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "    Args:\n",
    "\n",
    "        dim1: int, for both src and tgt masking, this must be target sequence\n",
    "              length\n",
    "\n",
    "        dim2: int, for src masking this must be encoder sequence length (i.e. \n",
    "              the length of the input sequence to the model), \n",
    "              and for tgt masking, this must be target sequence length \n",
    "\n",
    "\n",
    "    Return:\n",
    "\n",
    "        A Tensor of shape [dim1, dim2]\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1).to(device)\n",
    "\n",
    "\n",
    "def get_indices_input_target(num_obs, input_len, step_size, forecast_horizon, target_len):\n",
    "        \"\"\"\n",
    "        Produce all the start and end index positions of all sub-sequences.\n",
    "        The indices will be used to split the data into sub-sequences on which \n",
    "        the models will be trained. \n",
    "\n",
    "        Returns a tuple with four elements:\n",
    "        1) The index position of the first element to be included in the input sequence\n",
    "        2) The index position of the last element to be included in the input sequence\n",
    "        3) The index position of the first element to be included in the target sequence\n",
    "        4) The index position of the last element to be included in the target sequence\n",
    "\n",
    "        \n",
    "        Args:\n",
    "            num_obs (int): Number of observations in the entire dataset for which\n",
    "                            indices must be generated.\n",
    "\n",
    "            input_len (int): Length of the input sequence (a sub-sequence of \n",
    "                             of the entire data sequence)\n",
    "\n",
    "            step_size (int): Size of each step as the data sequence is traversed.\n",
    "                             If 1, the first sub-sequence will be indices 0-input_len, \n",
    "                             and the next will be 1-input_len.\n",
    "\n",
    "            forecast_horizon (int): How many index positions is the target away from\n",
    "                                    the last index position of the input sequence?\n",
    "                                    If forecast_horizon=1, and the input sequence\n",
    "                                    is data[0:10], the target will be data[11:taget_len].\n",
    "\n",
    "            target_len (int): Length of the target / output sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        input_len = round(input_len) # just a precaution\n",
    "        start_position = 0\n",
    "        stop_position = num_obs-1 # because of 0 indexing\n",
    "        \n",
    "        subseq_first_idx = start_position\n",
    "        subseq_last_idx = start_position + input_len\n",
    "        target_first_idx = subseq_last_idx + forecast_horizon\n",
    "        target_last_idx = target_first_idx + target_len \n",
    "        Log.debug(\"target_last_idx is {}\".format(target_last_idx))\n",
    "        Log.debug(\"stop_position is {}\".format(stop_position))\n",
    "        indices = []\n",
    "        while target_last_idx <= stop_position:\n",
    "            indices.append((subseq_first_idx, subseq_last_idx, target_first_idx, target_last_idx))\n",
    "            subseq_first_idx += step_size\n",
    "            subseq_last_idx += step_size\n",
    "            target_first_idx = subseq_last_idx + forecast_horizon\n",
    "            target_last_idx = target_first_idx + target_len\n",
    "\n",
    "        return indices\n",
    "\n",
    "def get_indices_entire_sequence(data: pd.DataFrame, window_size: int, step_size: int) -> list:\n",
    "        \"\"\"\n",
    "        Produce all the start and end index positions that is needed to produce\n",
    "        the sub-sequences. \n",
    "\n",
    "        Returns a list of tuples. Each tuple is (start_idx, end_idx) of a sub-\n",
    "        sequence. These tuples should be used to slice the dataset into sub-\n",
    "        sequences. These sub-sequences should then be passed into a function\n",
    "        that slices them into input and target sequences. \n",
    "        \n",
    "        Args:\n",
    "            num_obs (int): Number of observations (time steps) in the entire \n",
    "                           dataset for which indices must be generated, e.g. \n",
    "                           len(data)\n",
    "\n",
    "            window_size (int): The desired length of each sub-sequence. Should be\n",
    "                               (input_sequence_length + target_sequence_length)\n",
    "                               E.g. if you want the model to consider the past 100\n",
    "                               time steps in order to predict the future 50 \n",
    "                               time steps, window_size = 100+50 = 150\n",
    "\n",
    "            step_size (int): Size of each step as the data sequence is traversed \n",
    "                             by the moving window.\n",
    "                             If 1, the first sub-sequence will be [0:window_size], \n",
    "                             and the next will be [1:window_size].\n",
    "\n",
    "        Return:\n",
    "            indices: a list of tuples\n",
    "        \"\"\"\n",
    "\n",
    "        stop_position = len(data)-1 # 1- because of 0 indexing\n",
    "        \n",
    "        # Start the first sub-sequence at index position 0\n",
    "        subseq_first_idx = 0\n",
    "        \n",
    "        subseq_last_idx = window_size\n",
    "        \n",
    "        indices = []\n",
    "        \n",
    "        while subseq_last_idx <= stop_position:\n",
    "\n",
    "            indices.append((subseq_first_idx, subseq_last_idx))\n",
    "            \n",
    "            subseq_first_idx += step_size\n",
    "            \n",
    "            subseq_last_idx += step_size\n",
    "\n",
    "        return indices\n",
    "\n",
    "\n",
    "def read_data(\n",
    "          file_data_path: Union[str, Path],  \n",
    "          timestamp_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from csv file and return pd.Dataframe object\n",
    "\n",
    "    Args:\n",
    "\n",
    "        data_dir: str or Path object specifying the path to the directory \n",
    "                  containing the data\n",
    "\n",
    "        target_col_name: str, the name of the column containing the target variable\n",
    "\n",
    "        timestamp_col_name: str, the name of the column or named index \n",
    "                            containing the timestamps\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that `data_dir` is a Path object\n",
    "    file_data_path = Path(file_data_path)\n",
    "\n",
    "\n",
    "\n",
    "    Log.debug(\"Reading file in {}\".format(file_data_path))\n",
    "\n",
    "    data = pd.read_csv(\n",
    "        file_data_path, \n",
    "        parse_dates=[timestamp_col_name], \n",
    "        index_col=[timestamp_col_name], \n",
    "        infer_datetime_format=True,\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    # Make sure all \"n/e\" values have been removed from df. \n",
    "    if is_ne_in_df(data):\n",
    "        raise ValueError(\"data frame contains 'n/e' values. These must be handled\")\n",
    "\n",
    "    data = to_numeric_and_downcast_data(data)\n",
    "\n",
    "    # Make sure data is in ascending order by timestamp\n",
    "    data.sort_values(by=[timestamp_col_name], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def is_ne_in_df(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Some raw data files contain cells with \"n/e\". This function checks whether\n",
    "    any column in a df contains a cell with \"n/e\". Returns False if no columns\n",
    "    contain \"n/e\", True otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    for col in df.columns:\n",
    "\n",
    "        true_bool = (df[col] == \"n/e\")\n",
    "\n",
    "        if any(true_bool):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def to_numeric_and_downcast_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Downcast columns in df to smallest possible version of it's existing data\n",
    "    type\n",
    "    \"\"\"\n",
    "    fcols = df.select_dtypes('float').columns\n",
    "    \n",
    "    icols = df.select_dtypes('integer').columns\n",
    "\n",
    "    df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float')\n",
    "    \n",
    "    df[icols] = df[icols].apply(pd.to_numeric, downcast='integer')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding in Transformers: Understanding the PositionalEncoder Class\n",
    "\n",
    "In Natural Language Processing (NLP), Transformers play a crucial role in various tasks such as language translation and text analysis. A fundamental concept within Transformers is positional encoding, which imparts sequential information to the input data.\n",
    "\n",
    "This notebook explores the PositionalEncoder class, a vital component for incorporating positional encoding in Transformer models. Derived from PyTorch's Transformer tutorial, this class enriches input embeddings with positional information, enabling Transformers to understand the order of tokens within a sequence.\n",
    "\n",
    "Through this notebook, we'll dissect the PositionalEncoder class, understand its functionality, and illustrate its role in enhancing Transformer performance in NLP tasks.\n",
    "\n",
    "Let's dive into positional encoding with the PositionalEncoder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The authors of the original transformer paper describe very succinctly what \n",
    "    the positional encoding layer does and why it is needed:\n",
    "    \n",
    "    \"Since our model contains no recurrence and no convolution, in order for the \n",
    "    model to make use of the order of the sequence, we must inject some \n",
    "    information about the relative or absolute position of the tokens in the \n",
    "    sequence.\" (Vaswani et al, 2017)\n",
    "    Adapted from: \n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        dropout: float=0.1, \n",
    "        max_seq_len: int=5000, \n",
    "        d_model: int=512,\n",
    "        batch_first: bool=False\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "                     (Vaswani et al, 2017)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # adapted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        if self.batch_first:\n",
    "            pe = torch.zeros(1, max_seq_len, d_model)\n",
    "            \n",
    "            pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "            \n",
    "            pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        \n",
    "            pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "            pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or \n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            x = x + self.pe[:,:x.size(1)]\n",
    "        else:\n",
    "            x = x + self.pe[:x.size(0)]\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Forecasting with TimeSeriesTransformer Class\n",
    "\n",
    "Time series forecasting is a critical task in various domains, from finance to weather prediction. With the advent of Transformer architectures, the landscape of time series forecasting has witnessed significant advancements.\n",
    "\n",
    "In this notebook, we'll explore the TimeSeriesTransformer class, which implements a Transformer model tailored for time series forecasting. Inspired by the work of Wu et al. (2020), this class incorporates key concepts from the Transformer architecture to effectively model temporal data.\n",
    "\n",
    "The TimeSeriesTransformer class is designed with flexibility and performance in mind. It leverages principles from Vaswani et al. (2017) and PyTorch's Transformer module to create a robust framework for time series prediction. Unlike traditional approaches, this class separates input layers, positional encoding layers, and linear mapping layers from the encoder and decoder, enhancing modularity and usability.\n",
    "\n",
    "For a detailed explanation of the code and its underlying concepts, refer to the accompanying article available here.\n",
    "\n",
    "Let's delve into time series forecasting using the TimeSeriesTransformer class and explore its capabilities in modeling and predicting temporal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn \n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a transformer model that can be used for times series\n",
    "    forecasting. This time series transformer model is based on the paper by\n",
    "    Wu et al (2020) [1]. The paper will be referred to as \"the paper\".\n",
    "\n",
    "    A detailed description of the code can be found in my article here:\n",
    "\n",
    "    https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
    "\n",
    "    In cases where the paper does not specify what value was used for a specific\n",
    "    configuration/hyperparameter, this class uses the values from Vaswani et al\n",
    "    (2017) [2] or from PyTorch source code.\n",
    "\n",
    "    Unlike the paper, this class assumes that input layers, positional encoding \n",
    "    layers and linear mapping layers are separate from the encoder and decoder, \n",
    "    i.e. the encoder and decoder only do what is depicted as their sub-layers \n",
    "    in the paper. For practical purposes, this assumption does not make a \n",
    "    difference - it merely means that the linear and positional encoding layers\n",
    "    are implemented inside the present class and not inside the \n",
    "    Encoder() and Decoder() classes.\n",
    "\n",
    "    [1] Wu, N., Green, B., Ben, X., O'banion, S. (2020). \n",
    "    'Deep Transformer Models for Time Series Forecasting: \n",
    "    The Influenza Prevalence Case'. \n",
    "    arXiv:2001.08317 [cs, stat] [Preprint]. \n",
    "    Available at: http://arxiv.org/abs/2001.08317 (Accessed: 9 March 2022).\n",
    "\n",
    "    [2] Vaswani, A. et al. (2017) \n",
    "    'Attention Is All You Need'.\n",
    "    arXiv:1706.03762 [cs] [Preprint]. \n",
    "    Available at: http://arxiv.org/abs/1706.03762 (Accessed: 9 March 2022).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        input_size: int,\n",
    "        dec_seq_len: int,\n",
    "        batch_first: bool,\n",
    "        out_seq_len: int=58,\n",
    "        dim_val: int=512,  \n",
    "        n_encoder_layers: int=4,\n",
    "        n_decoder_layers: int=4,\n",
    "        n_heads: int=8,\n",
    "        dropout_encoder: float=0.2, \n",
    "        dropout_decoder: float=0.2,\n",
    "        dropout_pos_enc: float=0.1,\n",
    "        dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048,\n",
    "        num_predicted_features: int=1\n",
    "        ): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer \n",
    "                                     of the decoder\n",
    "\n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 because we're\n",
    "                                    only forecasting FCR-N prices in DK2, but in\n",
    "                                    we wanted to also predict FCR-D with the same\n",
    "                                    model, num_predicted_features should be 2.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        #print(\"input_size is: {}\".format(input_size))\n",
    "        #print(\"dim_val is: {}\".format(dim_val))\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size, \n",
    "            out_features=dim_val \n",
    "            )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=input_size,\n",
    "            out_features=dim_val\n",
    "            )  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=dim_val, \n",
    "            out_features=num_predicted_features\n",
    "            )\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(\n",
    "            d_model=dim_val,\n",
    "            dropout=dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        # The encoder layer used in the paper is identical to the one used by\n",
    "        # Vaswani et al (2017) on which the PyTorch module is based.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, \n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerEncoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=dim_val,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout_decoder,\n",
    "            batch_first=batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the decoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerDecoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers, \n",
    "            norm=None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "\n",
    "            src: the encoder's output sequence. Shape: (S,E) for unbatched input, \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "\n",
    "            tgt: the sequence to the decoder. Shape: (T,E) for unbatched input, \n",
    "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if \n",
    "                 batch_first=True, where T is the target sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "\n",
    "            src_mask: the mask for the src sequence to prevent the model from \n",
    "                      using data points from the target sequence\n",
    "\n",
    "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "        # Masking is only needed in the encoder if input sequences are padded\n",
    "        # which they are not in this time series use case, because all my\n",
    "        # input sequences are naturally of the same length. \n",
    "        # (https://github.com/huggingface/transformers/issues/4083)\n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "\n",
    "        # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
    "\n",
    "        #if src_mask is not None:\n",
    "            #print(\"From model.forward(): Size of src_mask: {}\".format(src_mask.size()))\n",
    "        #if tgt_mask is not None:\n",
    "            #print(\"From model.forward(): Size of tgt_mask: {}\".format(tgt_mask.size()))\n",
    "\n",
    "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "\n",
    "        #print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
    "\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
    "\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-based Time Series Dataset with TransformerDataset Class\n",
    "\n",
    "The TransformerDataset class plays a crucial role in preparing time series data for training Transformer models. This class, designed as a PyTorch Dataset, facilitates the slicing and formatting of time series sequences into appropriate input and target pairs for the Transformer model.\n",
    "\n",
    "In this notebook, we'll explore the functionalities of the TransformerDataset class and understand how it streamlines the preprocessing of time series data. Inspired by the work of Wu et al. (2020) and the principles outlined in Vaswani et al. (2017), this class adheres to best practices in handling temporal data for Transformer-based models.\n",
    "\n",
    "The TransformerDataset class accepts raw time series data and splits it into sequences suitable for training. Through this notebook, we'll dive into its methods and demonstrate how to use it effectively in conjunction with Transformer models.\n",
    "\n",
    "Let's dive into the world of time series data preparation using the TransformerDataset class and unlock its potential for enhancing Transformer-based time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "class TransformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class used for transformer models.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        data: torch.tensor,\n",
    "        indices: list, \n",
    "        enc_seq_len: int, \n",
    "        dec_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            data: tensor, the entire train, validation or test data sequence \n",
    "                        before any slicing. If univariate, data.size() will be \n",
    "                        [number of samples, number of variables]\n",
    "                        where the number of variables will be equal to 1 + the number of\n",
    "                        exogenous variables. Number of exogenous variables would be 0\n",
    "                        if univariate.\n",
    "\n",
    "            indices: a list of tuples. Each tuple has two elements:\n",
    "                     1) the start index of a sub-sequence\n",
    "                     2) the end index of a sub-sequence. \n",
    "                     The sub-sequence is split into src, trg and trg_y later.  \n",
    "\n",
    "            enc_seq_len: int, the desired length of the input sequence given to the\n",
    "                     the first layer of the transformer model.\n",
    "\n",
    "            target_seq_len: int, the desired length of the target sequence (the output of the model)\n",
    "\n",
    "            target_idx: The index position of the target variable in data. Data\n",
    "                        is a 2D tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.indices = indices\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        Log.info(\"From get_src_trg: data size = {}\".format(data.size()))\n",
    "\n",
    "        self.enc_seq_len = enc_seq_len\n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        self.target_seq_len = target_seq_len\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple with 3 elements:\n",
    "        1) src (the encoder input)\n",
    "        2) trg (the decoder input)\n",
    "        3) trg_y (the target)\n",
    "        \"\"\"\n",
    "        # Get the first element of the i'th tuple in the list self.indicesasdfas\n",
    "        start_idx = self.indices[index][0]\n",
    "\n",
    "        # Get the second (and last) element of the i'th tuple in the list self.indices\n",
    "        end_idx = self.indices[index][1]\n",
    "\n",
    "        sequence = self.data[start_idx:end_idx]\n",
    "\n",
    "        #print(\"From __getitem__: sequence length = {}\".format(len(sequence)))\n",
    "\n",
    "        src, trg, trg_y = self.get_src_trg(\n",
    "            sequence=sequence,\n",
    "            enc_seq_len=self.enc_seq_len,\n",
    "            dec_seq_len=self.dec_seq_len,\n",
    "            target_seq_len=self.target_seq_len\n",
    "            )\n",
    "\n",
    "        return src, trg, trg_y\n",
    "    \n",
    "    def get_src_trg(\n",
    "        self,\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        dec_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "        sequences from a sequence. \n",
    "\n",
    "        Args:\n",
    "\n",
    "            sequence: tensor, a 1D tensor of length n where \n",
    "                    n = encoder input length + target sequence length  \n",
    "\n",
    "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "\n",
    "            target_seq_len: int, the desired length of the target sequence (the \n",
    "                            one against which the model output is compared)\n",
    "\n",
    "        Return: \n",
    "\n",
    "            src: tensor, 1D, used as input to the transformer model\n",
    "\n",
    "            trg: tensor, 1D, used as input to the transformer model\n",
    "\n",
    "            trg_y: tensor, 1D, the target sequence against which the model output\n",
    "                is compared when computing loss. \n",
    "        \n",
    "        \"\"\"\n",
    "        assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "        \n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. As per the paper, it must have the same dimension as the \n",
    "        # target sequence, and it must contain the last value of src, and all\n",
    "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "        \n",
    "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:]\n",
    "\n",
    "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "        return src, trg, trg_y.squeeze(-1) # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is for encoder-decoder type models in which the decoder requires an input, tgt, which - during training - is the target sequence. During inference, the values of tgt are unknown, and the values therefore have to be generated iteratively.  \n",
    "This function returns a prediction of length forecast_window for each batch in src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "\n",
    "def run_encoder_decoder_inference(\n",
    "    model: nn.Module, \n",
    "    src: torch.Tensor, \n",
    "    forecast_window: int,\n",
    "    batch_size: int,\n",
    "    device,\n",
    "    batch_first: bool=False\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    NB! This function is currently only tested on models that work with \n",
    "    batch_first = False\n",
    "\n",
    "    \n",
    "    NB! If you want the inference to be done without gradient calculation, \n",
    "    make sure to call this function inside the context manager torch.no_grad like:\n",
    "    with torch.no_grad:\n",
    "        run_encoder_decoder_inference()\n",
    "        \n",
    "    The context manager is intentionally not called inside this function to make\n",
    "    it usable in cases where the function is used to compute loss that must be \n",
    "    backpropagated during training and gradient calculation hence is required.\n",
    "    \n",
    "    If use_predicted_tgt = True:\n",
    "    To begin with, tgt is equal to the last value of src. Then, the last element\n",
    "    in the model's prediction is iteratively concatenated with tgt, such that \n",
    "    at each step in the for-loop, tgt's size increases by 1. Finally, tgt will\n",
    "    have the correct length (target sequence length) and the final prediction\n",
    "    will be produced and returned.\n",
    "    \n",
    "    Args:\n",
    "        model: An encoder-decoder type model where the decoder requires\n",
    "               target values as input. Should be set to evaluation mode before \n",
    "               passed to this function.\n",
    "               \n",
    "        src: The input to the model\n",
    "        \n",
    "        forecast_horizon: The desired length of the model's output, e.g. 58 if you\n",
    "                         want to predict the next 58 hours of FCR prices.\n",
    "                           \n",
    "        batch_size: batch size\n",
    "        \n",
    "        batch_first: If true, the shape of the model input should be \n",
    "                     [batch size, input sequence length, number of features].\n",
    "                     If false, [input sequence length, batch size, number of features]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Dimension of a batched model input that contains the target sequence values\n",
    "    target_seq_dim = 0 if batch_first == False else 1\n",
    "\n",
    "    # Take the last value of thetarget variable in all batches in src and make it tgt\n",
    "    # as per the Influenza paper\n",
    "    tgt = src[-1, :, 0] if batch_first == False else src[:, -1, 0] # shape [1, batch_size, 1]\n",
    "\n",
    "    # Change shape from [batch_size] to [1, batch_size, 1]\n",
    "    if batch_size == 1 and batch_first == False:\n",
    "        tgt = tgt.unsqueeze(0).unsqueeze(0) # change from [1] to [1, 1, 1]\n",
    "\n",
    "    # Change shape from [batch_size] to [1, batch_size, 1]\n",
    "    if batch_first == False and batch_size > 1:\n",
    "        tgt = tgt.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    # Iteratively concatenate tgt with the first element in the prediction\n",
    "    for _ in range(forecast_window-1):\n",
    "\n",
    "        # Create masks\n",
    "        dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "        dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_a,\n",
    "            device=device\n",
    "            )\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_b,\n",
    "            device=device\n",
    "            )\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model(src, tgt, src_mask, tgt_mask) \n",
    "\n",
    "        # If statement simply makes sure that the predicted value is \n",
    "        # extracted and reshaped correctly\n",
    "        if batch_first == False:\n",
    "\n",
    "            # Obtain the predicted value at t+1 where t is the last time step \n",
    "            # represented in tgt\n",
    "            last_predicted_value = prediction[-1, :, :] \n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Obtain predicted value\n",
    "            last_predicted_value = prediction[:, -1, :]\n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [batch_size, 1, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(-1)\n",
    "\n",
    "        # Detach the predicted element from the graph and concatenate with \n",
    "        # tgt in dimension 1 or 0\n",
    "        tgt = torch.cat((tgt, last_predicted_value.detach()), target_seq_dim)\n",
    "    \n",
    "    # Create masks\n",
    "    dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "    dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_a,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_b,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    # Make final prediction\n",
    "    final_prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an Encoder-Decoder Model for Time Series Forecasting\n",
    "\n",
    "In time series forecasting, training a model involves iteratively optimizing its parameters to minimize the discrepancy between predicted and actual values. This notebook focuses on training an encoder-decoder model, a powerful architecture widely used for time series prediction tasks.\n",
    "\n",
    "The train_encoder_decoder function presented here facilitates the training process by iteratively updating the model's parameters using backpropagation and gradient descent. It leverages the PyTorch framework to define and optimize the model, compute loss, and update parameters efficiently.\n",
    "\n",
    "Through this notebook, we'll delve into the intricacies of training an encoder-decoder model for time series forecasting. We'll explore the training procedure, including data loading, model initialization, loss computation, and parameter optimization. Additionally, we'll monitor the training progress and evaluate model performance using appropriate metrics.\n",
    "\n",
    "Let's embark on the journey of training an encoder-decoder model for time series forecasting and unlock its potential for accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn \n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_encoder_decoder(\n",
    "    model: nn.Module, \n",
    "    train_loader: DataLoader,\n",
    "    epochs: int,\n",
    "    output_sequence_length: int,\n",
    "    enc_seq_len: int,\n",
    "    batch_first: bool,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the encoder-decoder model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The encoder-decoder model.\n",
    "        train_loader (DataLoader): DataLoader containing the training data.\n",
    "        epochs (int): Number of epochs for training.\n",
    "        forecast_window (int): Length of the forecast window.\n",
    "        enc_seq_len (int): Length of the input sequence to the encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())  # Initialize Adam optimizer\n",
    "\n",
    "    criterion = torch.nn.MSELoss()  # Mean squared error loss\n",
    "    \n",
    "    # Iterate over all epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "\n",
    "        # Iterate over all (x,y) pairs in training dataloader\n",
    "        for i, (src, tgt, tgt_y) in enumerate(train_loader):\n",
    "            \n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            tgt_y = tgt_y.to(device)\n",
    "\n",
    "\n",
    "            if batch_first == False:\n",
    "                shape_before = src.shape\n",
    "                src = src.permute(1, 0, 2)\n",
    "                Log.debug(\"src shape changed from {} to {}\".format(shape_before, src.shape))\n",
    "\n",
    "                shape_before = tgt.shape\n",
    "                tgt = tgt.permute(1, 0, 2)\n",
    "                Log.debug(\"src shape changed from {} to {}\".format(shape_before, tgt.shape))\n",
    "\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate masks\n",
    "            # tgt_mask = generate_square_subsequent_mask(\n",
    "            #     dim1=forecast_window,\n",
    "            #     dim2=forecast_window,\n",
    "            #     device=device\n",
    "            # )\n",
    "\n",
    "            # src_mask = generate_square_subsequent_mask(\n",
    "            #     dim1=forecast_window,\n",
    "            #     dim2=enc_seq_len,\n",
    "            #     device=device\n",
    "            # )\n",
    "\n",
    "            src_mask = generate_square_subsequent_mask(\n",
    "                dim1=output_sequence_length,\n",
    "                dim2=enc_seq_len,\n",
    "                device=device\n",
    "                )\n",
    "\n",
    "            # Make tgt mask for decoder with size:\n",
    "            # [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "            tgt_mask = generate_square_subsequent_mask( \n",
    "                dim1=output_sequence_length,\n",
    "                dim2=output_sequence_length,\n",
    "                device=device\n",
    "                )\n",
    "\n",
    "            # Make forecasts\n",
    "            prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "            if batch_first == False:\n",
    "                shape_before = prediction.shape\n",
    "                prediction = prediction.permute(1, 0, 2)\n",
    "                Log.debug(\"prediction shape changed from {} to {}\".format(shape_before, prediction.shape))\n",
    "\n",
    "            # Compute and backprop loss\n",
    "            loss = criterion(tgt_y.squeeze(), prediction.squeeze())\n",
    "            loss.backward()\n",
    "\n",
    "            # Take optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss +=  loss.item()\n",
    "\n",
    "        last_loss = running_loss / len(train_loader)\n",
    "        Log.info(f\"Training: Epoch {epoch + 1}/{epochs}, Loss: {last_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference with Encoder-Decoder Model for Time Series Forecasting\n",
    "\n",
    "In time series forecasting, making predictions on new data points is crucial for decision-making and planning. This notebook focuses on performing inference using an encoder-decoder model, a powerful architecture known for its effectiveness in time series prediction tasks.\n",
    "\n",
    "The run_encoder_decoder_inference function presented here facilitates the inference process by iteratively generating predictions for a given input sequence. Leveraging PyTorch's capabilities, this function orchestrates the model's forward pass and generates forecasts iteratively, considering the temporal dependencies encoded in the input data.\n",
    "\n",
    "Through this notebook, we'll explore the nuances of performing inference with an encoder-decoder model for time series forecasting. We'll delve into the function's implementation, understand its inner workings, and demonstrate how to use it effectively to generate accurate predictions.\n",
    "\n",
    "Let's embark on the journey of performing inference with an encoder-decoder model for time series forecasting and unlock its potential for making insightful predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "\n",
    "def run_encoder_decoder_inference(\n",
    "    model: nn.Module, \n",
    "    src: torch.Tensor, \n",
    "    forecast_window: int,\n",
    "    batch_size: int,\n",
    "    device,\n",
    "    batch_first: bool\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    NB! This function is currently only tested on models that work with \n",
    "    batch_first = False\n",
    "    \n",
    "    This function is for encoder-decoder type models in which the decoder requires\n",
    "    an input, tgt, which - during training - is the target sequence. During inference,\n",
    "    the values of tgt are unknown, and the values therefore have to be generated\n",
    "    iteratively.  \n",
    "    \n",
    "    This function returns a prediction of length forecast_window for each batch in src\n",
    "    \n",
    "    NB! If you want the inference to be done without gradient calculation, \n",
    "    make sure to call this function inside the context manager torch.no_grad like:\n",
    "    with torch.no_grad:\n",
    "        run_encoder_decoder_inference()\n",
    "        \n",
    "    The context manager is intentionally not called inside this function to make\n",
    "    it usable in cases where the function is used to compute loss that must be \n",
    "    backpropagated during training and gradient calculation hence is required.\n",
    "    \n",
    "    If use_predicted_tgt = True:\n",
    "    To begin with, tgt is equal to the last value of src. Then, the last element\n",
    "    in the model's prediction is iteratively concatenated with tgt, such that \n",
    "    at each step in the for-loop, tgt's size increases by 1. Finally, tgt will\n",
    "    have the correct length (target sequence length) and the final prediction\n",
    "    will be produced and returned.\n",
    "    \n",
    "    Args:\n",
    "        model: An encoder-decoder type model where the decoder requires\n",
    "               target values as input. Should be set to evaluation mode before \n",
    "               passed to this function.\n",
    "               \n",
    "        src: The input to the model\n",
    "        \n",
    "        forecast_horizon: The desired length of the model's output, e.g. 58 if you\n",
    "                         want to predict the next 58 hours of FCR prices.\n",
    "                           \n",
    "        batch_size: batch size\n",
    "        \n",
    "        batch_first: If true, the shape of the model input should be \n",
    "                     [batch size, input sequence length, number of features].\n",
    "                     If false, [input sequence length, batch size, number of features]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Dimension of a batched model input that contains the target sequence values\n",
    "    target_seq_dim = 0 if batch_first == False else 1\n",
    "\n",
    "    # Take the last value of thetarget variable in all batches in src and make it tgt\n",
    "    # as per the Influenza paper\n",
    "    tgt = src[-1, :, 0] if batch_first == False else src[:, -1, 0] # shape [1, batch_size, 1]\n",
    "\n",
    "    # Change shape from [batch_size] to [1, batch_size, 1]\n",
    "    if batch_size == 1 and batch_first == False:\n",
    "        tgt = tgt.unsqueeze(0).unsqueeze(0) # change from [1] to [1, 1, 1]\n",
    "\n",
    "    # Change shape from [batch_size] to [1, batch_size, 1]\n",
    "    if batch_first == False and batch_size > 1:\n",
    "        tgt = tgt.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    # Iteratively concatenate tgt with the first element in the prediction\n",
    "    for _ in range(forecast_window-1):\n",
    "\n",
    "        # Create masks\n",
    "        dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "        dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_a,\n",
    "            device=device\n",
    "            )\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(\n",
    "            dim1=dim_a,\n",
    "            dim2=dim_b,\n",
    "            device=device\n",
    "            )\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model(src, tgt, src_mask, tgt_mask) \n",
    "\n",
    "        # If statement simply makes sure that the predicted value is \n",
    "        # extracted and reshaped correctly\n",
    "        if batch_first == False:\n",
    "\n",
    "            # Obtain the predicted value at t+1 where t is the last time step \n",
    "            # represented in tgt\n",
    "            last_predicted_value = prediction[-1, :, :] \n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Obtain predicted value\n",
    "            last_predicted_value = prediction[:, -1, :]\n",
    "\n",
    "            # Reshape from [batch_size, 1] --> [batch_size, 1, 1]\n",
    "            last_predicted_value = last_predicted_value.unsqueeze(-1)\n",
    "\n",
    "        # Detach the predicted element from the graph and concatenate with \n",
    "        # tgt in dimension 1 or 0\n",
    "        tgt = torch.cat((tgt, last_predicted_value.detach()), target_seq_dim)\n",
    "    \n",
    "    # Create masks\n",
    "    dim_a = tgt.shape[1] if batch_first == True else tgt.shape[0]\n",
    "\n",
    "    dim_b = src.shape[1] if batch_first == True else src.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_a,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    src_mask = generate_square_subsequent_mask(\n",
    "        dim1=dim_a,\n",
    "        dim2=dim_b,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "    # Make final prediction\n",
    "    final_prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Forecasting with Transformer Models\n",
    "\n",
    "Time series forecasting is a critical task across various domains, including finance, energy, and weather prediction. Transformer models, initially introduced for natural language processing, have shown remarkable performance in handling sequential data, making them an attractive choice for time series forecasting tasks.\n",
    "\n",
    "This notebook explores the application of transformer models for time series forecasting. We'll dive into the implementation of a Transformer-based architecture specifically designed for predicting future values in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Reading file in ../data/dfs_merged_upload.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/q3z8t_dx27188q6j83nlvg_h0000gn/T/ipykernel_75835/2389301303.py:157: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  data = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] From get_src_trg: data size = torch.Size([41387, 1])\n",
      "[DEBUG] src shape changed from torch.Size([128, 153, 1]) to torch.Size([153, 128, 1])\n",
      "[DEBUG] src shape changed from torch.Size([128, 48, 1]) to torch.Size([48, 128, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dellacqp/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] prediction shape changed from torch.Size([48, 128, 1]) to torch.Size([128, 48, 1])\n",
      "[DEBUG] src shape changed from torch.Size([128, 153, 1]) to torch.Size([153, 128, 1])\n",
      "[DEBUG] src shape changed from torch.Size([128, 48, 1]) to torch.Size([48, 128, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 74\u001b[0m\n\u001b[1;32m     63\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(training_data, batch_size)\n\u001b[1;32m     66\u001b[0m model \u001b[38;5;241m=\u001b[39m TimeSeriesTransformer(\n\u001b[1;32m     67\u001b[0m     input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(input_variables),\n\u001b[1;32m     68\u001b[0m     dec_seq_len\u001b[38;5;241m=\u001b[39menc_seq_len,\n\u001b[1;32m     69\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39mbatch_first,\n\u001b[1;32m     70\u001b[0m     num_predicted_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     71\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtrain_encoder_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_sequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_sequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 84\u001b[0m, in \u001b[0;36mtrain_encoder_decoder\u001b[0;34m(model, train_loader, epochs, output_sequence_length, enc_seq_len, batch_first, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m generate_square_subsequent_mask( \n\u001b[1;32m     78\u001b[0m     dim1\u001b[38;5;241m=\u001b[39moutput_sequence_length,\n\u001b[1;32m     79\u001b[0m     dim2\u001b[38;5;241m=\u001b[39moutput_sequence_length,\n\u001b[1;32m     80\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Make forecasts\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_first \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     shape_before \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 221\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    212\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_input_layer(tgt) \u001b[38;5;66;03m# src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m#print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m#if src_mask is not None:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m#print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Pass through linear mapping\u001b[39;00m\n\u001b[1;32m    231\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_mapping(decoder_output) \u001b[38;5;66;03m# shape [batch_size, target seq len]\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:465\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    462\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 465\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:856\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m--> 856\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mha_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    857\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:874\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    873\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 874\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:5336\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5335\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 5336\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5338\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:4869\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4867\u001b[0m     b_q, b_kv \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msplit([E, E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   4868\u001b[0m q_proj \u001b[38;5;241m=\u001b[39m linear(q, w_q, b_q)\n\u001b[0;32m-> 4869\u001b[0m kv_proj \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_kv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4870\u001b[0m \u001b[38;5;66;03m# reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4871\u001b[0m kv_proj \u001b[38;5;241m=\u001b[39m kv_proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m2\u001b[39m, E))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dataset as ds\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Log.setLogLevel(level=logging.DEBUG)\n",
    "Log.useLogging(False)\n",
    "\n",
    "# Hyperparams\n",
    "epochs = 2\n",
    "test_size = 0.1\n",
    "batch_size = 128\n",
    "target_col_name = \"FCR_N_PriceEUR\"\n",
    "timestamp_col = \"timestamp\"\n",
    "\n",
    "## Params\n",
    "dim_val = 512\n",
    "n_heads = 8\n",
    "\n",
    "dec_seq_len = 92 # length of input given to decoder\n",
    "enc_seq_len = 153 # length of input given to encoder\n",
    "output_sequence_length = 48 # target sequence length. If hourly data and length = 48, you predict 2 days ahead\n",
    "window_size = enc_seq_len + output_sequence_length # used to slice data into sub-sequences\n",
    "step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n",
    "\n",
    "max_seq_len = enc_seq_len\n",
    "batch_first = False\n",
    "\n",
    "# Define input variables \n",
    "exogenous_vars = [] # should contain strings. Each string must correspond to a column name\n",
    "input_variables = [target_col_name] + exogenous_vars\n",
    "target_idx = 0 # index position of target in batched trg_y\n",
    "\n",
    "input_size = len(input_variables)\n",
    "\n",
    "# Read data\n",
    "data = read_data(file_data_path='../data/dfs_merged_upload.csv', timestamp_col_name=timestamp_col)\n",
    "\n",
    "# Remove test data from dataset\n",
    "training_data = data[:-(round(len(data)*test_size))]\n",
    "\n",
    "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chunkc. \n",
    "# Should be training data indices only\n",
    "training_indices = get_indices_entire_sequence(\n",
    "    data=training_data, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size)\n",
    "\n",
    "# Making instance of custom dataset class\n",
    "training_data = TransformerDataset(\n",
    "    data=torch.tensor(training_data[input_variables].values).float(),\n",
    "    indices=training_indices,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dec_seq_len=dec_seq_len,\n",
    "    target_seq_len=output_sequence_length\n",
    "    )\n",
    "\n",
    "# Making dataloader\n",
    "train_loader = DataLoader(training_data, batch_size)\n",
    "\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    input_size=len(input_variables),\n",
    "    dec_seq_len=enc_seq_len,\n",
    "    batch_first=batch_first,\n",
    "    num_predicted_features=1\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "train_encoder_decoder(model=model, train_loader=train_loader, epochs=epochs, output_sequence_length=output_sequence_length, enc_seq_len=enc_seq_len, batch_first=batch_first, device=device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
